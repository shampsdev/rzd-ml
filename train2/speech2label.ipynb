{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество доступных GPU: 1\n",
      "GPU 0: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import librosa\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Количество доступных GPU: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA не доступна.\")\n",
    "\n",
    "# --- Настройки ---\n",
    "audio_folder_noise = './dataset/hr_bot_noise'\n",
    "json_file_noise = './dataset/annotation/hr_bot_noise.json'\n",
    "audio_folder_clear = './dataset/hr_bot_synt'\n",
    "json_file_clear = './dataset/annotation/hr_bot_synt.json'\n",
    "model_output_dir = \"./trained\"\n",
    "os.makedirs(model_output_dir, exist_ok=True)\n",
    "\n",
    "# --- Гиперпараметры ---\n",
    "input_dim = 128  # Размерность MFCC\n",
    "hidden_dim = 2048  # Увеличение размерности скрытых слоев\n",
    "output_dim = 30  # Количество классов\n",
    "batch_size = 4  # Размер пакета\n",
    "learning_rate = 0.000001\n",
    "num_epochs = 1  # Увеличиваем количество эпох для более глубокого обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Функции ---\n",
    "def convert_texts_to_labels(texts):\n",
    "    unique_texts = list(set(texts))\n",
    "    text_to_index = {text: index for index, text in enumerate(unique_texts)}\n",
    "    labels = [text_to_index[text] for text in texts]\n",
    "    return torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "def extract_mfcc(audio_path):\n",
    "    \"\"\"Извлекает MFCC из аудиофайла.\"\"\"\n",
    "    try:\n",
    "        waveform, sample_rate = librosa.load(audio_path, sr=16000)\n",
    "        mfcc = librosa.feature.mfcc(y=waveform, sr=sample_rate, n_mfcc=input_dim)\n",
    "        return mfcc.T  # Транспонируем для удобства\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading audio: {audio_path}, Error: {e}\")\n",
    "        return None  \n",
    "\n",
    "def pad_collate(batch):\n",
    "    mfccs, texts = zip(*batch)\n",
    "    mfccs = [m for m in mfccs if m is not None]\n",
    "    if len(mfccs) == 0:\n",
    "        return None, None\n",
    "    mfccs_padded = pad_sequence(mfccs, batch_first=True, padding_value=0.0)\n",
    "    return mfccs_padded, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speech2label_classes import SpeechDataset, LSTMSpeechRecognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Создание датасета на шумных данных\n",
    "full_dataset_noise = SpeechDataset(audio_folder_noise, json_file_noise, transform=extract_mfcc)\n",
    "\n",
    "# Определение размеров подвыборок для шумных данных\n",
    "train_size = int(0.8 * len(full_dataset_noise))\n",
    "val_size = len(full_dataset_noise) - train_size\n",
    "\n",
    "# Разделение на обучающую и проверочную выборки\n",
    "train_dataset, val_dataset = random_split(full_dataset_noise, [train_size, val_size])\n",
    "\n",
    "# Создание загрузчиков данных\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=pad_collate)\n",
    "\n",
    "# Создание и подготовка модели\n",
    "model = LSTMSpeechRecognizer(input_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)  # L2 регуляризация\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1, Batch: 1/215, Loss: 3.4175\n",
      "Epoch: 1/1, Batch: 11/215, Loss: 3.4475\n",
      "Epoch: 1/1, Batch: 21/215, Loss: 3.4409\n",
      "Epoch: 1/1, Batch: 31/215, Loss: 3.3868\n",
      "Epoch: 1/1, Batch: 41/215, Loss: 3.3203\n",
      "Epoch: 1/1, Batch: 51/215, Loss: 3.3844\n",
      "Epoch: 1/1, Batch: 61/215, Loss: 3.2622\n",
      "Epoch: 1/1, Batch: 71/215, Loss: 3.2676\n",
      "Epoch: 1/1, Batch: 81/215, Loss: 3.1825\n",
      "Epoch: 1/1, Batch: 91/215, Loss: 3.2799\n",
      "Epoch: 1/1, Batch: 101/215, Loss: 3.1673\n",
      "Epoch: 1/1, Batch: 111/215, Loss: 3.2350\n",
      "Epoch: 1/1, Batch: 121/215, Loss: 3.0704\n",
      "Epoch: 1/1, Batch: 131/215, Loss: 3.0753\n",
      "Epoch: 1/1, Batch: 141/215, Loss: 3.0667\n",
      "Epoch: 1/1, Batch: 151/215, Loss: 3.0720\n",
      "Epoch: 1/1, Batch: 161/215, Loss: 2.9616\n",
      "Epoch: 1/1, Batch: 171/215, Loss: 3.0152\n",
      "Epoch: 1/1, Batch: 181/215, Loss: 2.8813\n",
      "Epoch: 1/1, Batch: 191/215, Loss: 2.8677\n",
      "Epoch: 1/1, Batch: 201/215, Loss: 2.9331\n",
      "Epoch: 1/1, Batch: 211/215, Loss: 2.8314\n",
      "Epoch: 1/1, Train Loss: 3.1829, Train Accuracy: 0.2047\n",
      "Epoch: 1/1, Validation Loss: 2.9131, Validation Accuracy: 0.2512\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Включаем режим обучения\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (mfcc, texts) in enumerate(train_loader):\n",
    "        if mfcc is None:\n",
    "            continue\n",
    "        \n",
    "        mfcc = mfcc.to(device)\n",
    "        labels = convert_texts_to_labels(texts).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(mfcc)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch: {epoch+1}/{num_epochs}, Batch: {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    epoch_loss = total_loss / len(train_loader)\n",
    "    epoch_accuracy = correct / total\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_accuracy)\n",
    "    print(f\"Epoch: {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n",
    "    # --- Валидация ---\n",
    "    model.eval()  # Включаем режим оценки\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():  # Отключаем градиенты для валидации\n",
    "        for batch_idx, (mfcc, texts) in enumerate(val_loader):\n",
    "            if mfcc is None:\n",
    "                continue\n",
    "            \n",
    "            mfcc = mfcc.to(device)\n",
    "            labels = convert_texts_to_labels(texts).to(device)\n",
    "\n",
    "            outputs = model(mfcc)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_epoch_loss = val_loss / len(val_loader)\n",
    "    val_epoch_accuracy = val_correct / val_total\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracies.append(val_epoch_accuracy)\n",
    "    print(f\"Epoch: {epoch+1}/{num_epochs}, Validation Loss: {val_epoch_loss:.4f}, Validation Accuracy: {val_epoch_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(model_output_dir, \"lstm_speech.pth\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
